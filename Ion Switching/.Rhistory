train_data$trend_spl<-0
train_data$piece<-NA
for(lBatch in c(0,2:5,6:9)){
if(lBatch %in% c(6:9)){
trend.lm<-lm(signal~poly(time_batch,2),data=subset(train_data,batches==lBatch))
}else{
trend.lm<-lm(signal~time_batch,data=subset(train_data,batches==lBatch))
}
cat("batch is",lBatch,'\n')
summary(trend.lm) %>% print()
train_data$trend_res[train_data$batches==lBatch] <- trend.lm$residuals
train_data$trend_spl[train_data$batches==lBatch]<-trend.lm$fitted.values
train_data$piece[train_data$batches==lBatch]<-paste(as.character(lBatch),"1")
}
ggplot(train_data %>% slice(seq(1,n(),300)),
aes(x=time_batch,y=signal,col=open_channels))+geom_point()+facet_grid(vars(batches))
ggplot(train_data %>% slice(seq(1,n(),400)),
aes(x=time_batch,y=signal,col=open_channels))+geom_point()+facet_grid(vars(batches))
inds=train_data$batches==1 & train_data$time_batch<=10
trend.lm<-lm(signal~poly(time_batch,2),data=subset(train_data,inds))
train_data$trend_res[inds]<-trend.lm$residuals
train_data$trend_spl[inds]<-trend.lm$fitted.values
train_data$piece[inds]<-"11"
summary(trend.lm) %>% print()
inds=train_data$batches==1 & train_data$time_batch>10
trend.lm<-lm(signal~time_batch,data=subset(train_data,inds))
train_data$trend_res[inds]<-trend.lm$residuals
train_data$trend_spl[inds]<-trend.lm$fitted.values
train_data$piece[inds]<-"12"
summary(trend.lm) %>% print()
cv5<- trainControl(method="cv",number=5,summaryFunction = f1)
train_ix<-createDataPartition(train_data$signal,p = 0.8,list = F,times = 1)
train<-train_data[train_ix,]
test<-train_data[-train_ix,]
f1 <- function (data, lev = NULL,model=NULL) {
cm<-as.matrix(table(actual=data$obs,predicted=data$pred))
diag<-diag(cm)
rowsums<-apply(cm,1,sum)
colsums<-apply(cm,2,sum)
precision <- ifelse(colsums==0,0,diag/colsums)
recall  <- ifelse(rowsums==0,0,diag/rowsums)
f1<- ifelse(precision+recall==0,0,2*precision*recall/(precision+recall))
f1_val <- mean(f1)
names(f1_val) <- c("F1")
f1_val
}
cv5<- trainControl(method="cv",number=5,summaryFunction = f1)
train_ix<-createDataPartition(train_data$signal,p = 0.8,list = F,times = 1)
train<-train_data[train_ix,]
test<-train_data[-train_ix,]
train_ix<-createDataPartition(train_data$signal,p = 0.8,list = F)
train_ix<-createDataPartition(train_data$signal,p = 0.8,list = F)[,1]
train<-train_data[train_ix,]
test<-train_data[-train_ix,]
lda.model02 <- train(open_channels~signal+batches,data=train,method="lda",family="binomial", trControl=cv5,verbosity=T,metric="F1")
lda.model02
lda.valid02<-predict(lda.model02,newdata = test)
f1(data.frame(obs=test$open_channels,pred=lda.valid02))
lda.model03 <- train(open_channels~trend_res+batches,data=train,method="lda",family="binomial", trControl=cv5,verbosity=T,metric="F1")
lda.model03
lda.valid03<-predict(lda.model03,newdata = test)
f1(data.frame(obs=test$open_channels,pred=lda.valid03))
ggplot(train_data %>% slice(seq(1,n(),400)),
aes(x=time_batch,y=signal-trend,col=open_channels))+geom_point()+facet_grid(vars(batches))
ggplot(train_data %>% slice(seq(1,n(),400)),
aes(x=time_batch,y=trend_res,col=open_channels))+geom_point()+facet_grid(vars(batches))
test_data$batches<- factor((test_data$time - 0.0001)%/%50,levels=levels(train_data$batches))
test_data$time_batch <- ((test_data$time - 0.0001)%%50)+0.0001
ggplot(test_data %>% slice(seq(1,n(),300)),
aes(x=time_batch,y=signal))+geom_point()+facet_grid(vars(batches))
View(test_data)
levels(train_data$batches)
(test_data$time - 0.0001)%/%50
test_data$batches<- factor((test_data$time - 0.0001)%/%50-10,levels=levels(train_data$batches))
ggplot(test_data %>% slice(seq(1,n(),300)),
aes(x=time_batch,y=signal))+geom_point()+facet_grid(vars(batches))
test_data$trend_res<-0
for(lbatch in c(0,1)){
inds=test_data$batches==lbatch & test_data$time_batch<=30
trend.lm<-lm(signal~poly(time_batch,2),data=subset(test_data,inds))
test_data$trend_res[inds]<-trend.lm$residuals
inds=test_data$batches==lbatch & test_data$time_batch>30
trend.lm<-lm(signal~poly(time_batch,2),data=subset(test_data,inds))
test_data$trend_res[inds]<-trend.lm$residuals
}
inds=test_data$batches==2
trend.lm<-lm(signal~poly(time_batch,2),data=subset(test_data,inds))
test_data$trend_res[inds]<-trend.lm$residuals
inds=test_data$batches==3
trend.lm<-lm(signal~time_batch,data=subset(test_data,inds))
test_data$trend_res[inds]<-trend.lm$residuals
ggplot(test_data %>% slice(seq(1,n(),300)),
aes(x=time_batch,y=trend_res))+geom_point()+facet_grid(vars(batches))
ggplot(train_data %>% slice(seq(1,n(),400)),
aes(x=time_batch,y=signal,col=open_channels))+geom_point()+facet_grid(vars(batches))
res=predict(lda.model03,newdata = test_data)
answer=read_csv(sample.path)
answer$time=format(answer$time,nsmall = 4)
#answer$open_channels=sample(0:10,answer$time %>% length(),replace = T)
answer$open_channels=res
write_csv(answer,paste0(data.path,"result.csv"))
lda.model03 <- train(open_channels~trend_res+batches,data=train_data,method="lda",family="binomial", trControl=cv5,verbosity=T,metric="F1")
test_data$batches[test_data$batches==0]=7
test_data$batches[test_data$batches==1]=4
test_data$batches[test_data$batches==2]=6
test_data$batches[test_data$batches==3]=0
res=predict(lda.model03,newdata = test_data)
res
answer=read_csv(sample.path)
answer$time=format(answer$time,nsmall = 4)
#answer$open_channels=sample(0:10,answer$time %>% length(),replace = T)
answer$open_channels=res
write_csv(answer,paste0(data.path,"result.csv"))
test_data$batches<- factor((test_data$time - 0.0001)%/%50-10,levels=levels(train_data$batches))
test_data$trend_res<-0
for(lbatch in c(0,1)){
inds=test_data$batches==lbatch & test_data$time_batch<=30
trend.lm<-lm(signal~poly(time_batch,2),data=subset(test_data,inds))
test_data$trend_res[inds]<-trend.lm$residuals
inds=test_data$batches==lbatch & test_data$time_batch>30
trend.lm<-lm(signal~poly(time_batch,2),data=subset(test_data,inds))
test_data$trend_res[inds]<-trend.lm$residuals
}
inds=test_data$batches==2
trend.lm<-lm(signal~poly(time_batch,2),data=subset(test_data,inds))
test_data$trend_res[inds]<-trend.lm$residuals
inds=test_data$batches==3
trend.lm<-lm(signal~time_batch,data=subset(test_data,inds))
test_data$trend_res[inds]<-trend.lm$residuals
tr=train_data %>% slice(seq(1,n(),600))
te=test_data %>% slice(seq(1,n(),400))
for(i in 0:3){
for(j in 0:9){
tmp=rbind(
tr %>% filter(batches==j) %>% select(time_batch,signal) %>% mutate(source='train'),
te %>% filter(batches==i) %>% select(time_batch,signal) %>% mutate(source='test')
)
ggplot(tmp %>% mutate(source=factor(source)),
aes(x=time_batch,y=signal))+geom_point()+facet_wrap(source~.)
}
}
for(i in 0:3){
for(j in 0:9){
tmp=rbind(
tr %>% filter(batches==j) %>% select(time_batch,signal) %>% mutate(source='train'),
te %>% filter(batches==i) %>% select(time_batch,signal) %>% mutate(source='test')
)
pl= ggplot(tmp %>% mutate(source=factor(source)),
aes(x=time_batch,y=signal))+geom_point()+facet_wrap(source~.)
ggsave(paste0('test batch = ',i,' train batch = ',j,'.png'),pl)
}
}
test_data$batches[test_data$batches==0]=6
test_data$batches[test_data$batches==1]=4
test_data$batches[test_data$batches==2]=6
test_data$batches[test_data$batches==3]=0
res=predict(lda.model03,newdata = test_data)
#writing_sample
answer=read_csv(sample.path)
answer$time=format(answer$time,nsmall = 4)
#answer$open_channels=sample(0:10,answer$time %>% length(),replace = T)
answer$open_channels=res
write_csv(answer,paste0(data.path,"result.csv"))
test_data$batches<- factor((test_data$time - 0.0001)%/%50-10,levels=levels(train_data$batches))
# use optimal batches
test_data$batches[test_data$batches==0]=7
test_data$batches[test_data$batches==1]=8
test_data$batches[test_data$batches==2]=6
test_data$batches[test_data$batches==3]=0
res=predict(lda.model03,newdata = test_data)
#writing_sample
answer=read_csv(sample.path)
answer$time=format(answer$time,nsmall = 4)
#answer$open_channels=sample(0:10,answer$time %>% length(),replace = T)
answer$open_channels=res
write_csv(answer,paste0(data.path,"result.csv"))
######## default batches
test_data$batches<- factor((test_data$time - 0.0001)%/%50-10,levels=levels(train_data$batches))
# use optimal batches
test_data$batches[test_data$batches==0]=7
test_data$batches[test_data$batches==1]=4
test_data$batches[test_data$batches==2]=6
test_data$batches[test_data$batches==3]=1
res=predict(lda.model03,newdata = test_data)
#writing_sample
answer=read_csv(sample.path)
answer$time=format(answer$time,nsmall = 4)
#answer$open_channels=sample(0:10,answer$time %>% length(),replace = T)
answer$open_channels=res
write_csv(answer,paste0(data.path,"result7461.csv"))
######## default batches
test_data$batches<- factor((test_data$time - 0.0001)%/%50-10,levels=levels(train_data$batches))
test_data$batches[test_data$batches==0]=7
test_data$batches[test_data$batches==1]=4
test_data$batches[test_data$batches==2]=6
test_data$batches[test_data$batches==3]=2
res=predict(lda.model03,newdata = test_data)
#writing_sample
answer=read_csv(sample.path)
answer$time=format(answer$time,nsmall = 4)
#answer$open_channels=sample(0:10,answer$time %>% length(),replace = T)
answer$open_channels=res
write_csv(answer,paste0(data.path,"result7462.csv"))
res=predict(lda.model03,newdata = test_data)
answer=read_csv(sample.path)
answer$time=format(answer$time,nsmall = 4)
#answer$open_channels=sample(0:10,answer$time %>% length(),replace = T)
answer$open_channels=res
write_csv(answer,paste0(data.path,"result7462.csv"))
#example from https://www.kaggle.com/jpbeconne/beginner-notebook
#https://www.kaggle.com/kittlein/clean-signal-and-naive-features
library(tidyverse)
library(data.table)
library(magrittr)
library(caret)
library(MLmetrics)
library(doSNOW)
library(data.table)
library(readr)
library(tidyverse)
library(xgboost)
library(RcppRoll)
library(dse)
library(KFAS)
library(wmtsa)
library(onehot)
install_github("andrewuhl/RollingWindow", dependencies=FALSE)
library(RollingWindow)
install.packages('tsfeatures', dependencies = TRUE)
library(tsfeatures)
install.packages(c("dse", "KFAS", "onehot", "RcppRoll", "tsfeatures", "wmtsa"))
install.packages(c("dse", "KFAS", "onehot", "RcppRoll", "tsfeatures", "wmtsa"))
devtools::install_github("andrewuhl/RollingWindow", dependencies=FALSE)
library(RollingWindow)
library(devtools)
install_github("andrewuhl/RollingWindow", dependencies=FALSE)
f1 <- function (data, lev = NULL,model=NULL) {
cm<-as.matrix(table(actual=data$obs,predicted=data$pred))
diag<-diag(cm)
rowsums<-apply(cm,1,sum)
colsums<-apply(cm,2,sum)
precision <- ifelse(colsums==0,0,diag/colsums)
recall  <- ifelse(rowsums==0,0,diag/rowsums)
f1<- ifelse(precision+recall==0,0,2*precision*recall/(precision+recall))
f1_val <- mean(f1)
names(f1_val) <- c("F1")
f1_val
}
data.path='D:/liverpool-ion-switching/'
train.path=paste0(data.path,'train','.csv')
test.path=paste0(data.path,'test','.csv')
sample.path=paste0(data.path,'sample_submission','.csv')
train_data<-read_csv(train.path,col_types = 'ddf')
test_data<-read_csv(test.path,col_types = 'dd')
train_data$batches<- as.factor((train_data$time - 0.0001)%/%50)
train_data$time_batch <- ((train_data$time - 0.0001)%%50)+0.0001
library(tidyverse)
#example from https://www.kaggle.com/jpbeconne/beginner-notebook
#https://www.kaggle.com/kittlein/clean-signal-and-naive-features
library(tidyverse)
library(data.table)
library(magrittr)
library(caret)
library(MLmetrics)
library(doSNOW)
library(data.table)
library(readr)
library(tidyverse)
library(xgboost)
library(RcppRoll)
library(dse)
library(KFAS)
library(wmtsa)
library(onehot)
library(devtools)
install_github("andrewuhl/RollingWindow", dependencies=FALSE)
library(RollingWindow)
install.packages('tsfeatures', dependencies = TRUE)
library(tsfeatures)
data.path='D:/liverpool-ion-switching/'
train.path=paste0(data.path,'train','.csv')
test.path=paste0(data.path,'test','.csv')
sample.path=paste0(data.path,'sample_submission','.csv')
train_data<-read_csv(train.path,col_types = 'ddf')
test_data<-read_csv(test.path,col_types = 'dd')
train_data$batches<- as.factor((train_data$time - 0.0001)%/%50)
train_data$time_batch <- ((train_data$time - 0.0001)%%50)+0.0001
install.packages(c("broom", "RCurl", "tibble", "timetk", "withr"))
devtools::install_github("rickhelmus/patRoon")
library(devtools)
install_github("andrewuhl/RollingWindow", dependencies=FALSE, force = T)
install_github("andrewuhl/RollingWindow",  force = T)
options(pkgType = "source")
install_github("andrewuhl/RollingWindow",  force = T)
#example from https://www.kaggle.com/jpbeconne/beginner-notebook
#https://www.kaggle.com/kittlein/clean-signal-and-naive-features
library(tidyverse)
library(data.table)
library(magrittr)
library(caret)
library(MLmetrics)
library(doSNOW)
library(data.table)
library(readr)
library(tidyverse)
library(xgboost)
library(RcppRoll)
library(dse)
library(KFAS)
library(wmtsa)
library(onehot)
#library(devtools)
#install_github("andrewuhl/RollingWindow")
#library(RollingWindow)
library(tsfeatures)
f1 <- function (data, lev = NULL,model=NULL) {
cm<-as.matrix(table(actual=data$obs,predicted=data$pred))
diag<-diag(cm)
rowsums<-apply(cm,1,sum)
colsums<-apply(cm,2,sum)
precision <- ifelse(colsums==0,0,diag/colsums)
recall  <- ifelse(rowsums==0,0,diag/rowsums)
f1<- ifelse(precision+recall==0,0,2*precision*recall/(precision+recall))
f1_val <- mean(f1)
names(f1_val) <- c("F1")
f1_val
}
data.path='D:/liverpool-ion-switching/'
train.path=paste0(data.path,'train','.csv')
test.path=paste0(data.path,'test','.csv')
sample.path=paste0(data.path,'sample_submission','.csv')
train_data<-read_csv(train.path,col_types = 'ddf')
test_data<-read_csv(test.path,col_types = 'dd')
train_data$batches<- as.factor((train_data$time - 0.0001)%/%50)
train_data$time_batch <- ((train_data$time - 0.0001)%%50)+0.0001
number_peaks <- function(x, window){
rollmax <- as.numeric(roll_max(x, window))
npeak <- sum(x > lead(rollmax, n = window) & x > lag(rollmax), na.rm = T)
return(npeak)
}
# fast removal of na values
Rep.Nas = function(DT) {
for (i in names(DT))
DT[is.na(get(i)), (i):=0]
}
decode=function(x){
bre=quantile(x, seq(0.1, 0.9, by=0.05))
clase=rep(NA, length(x))
for(j in 1:length(bre)){
indi= which(x >=  bre[j] & x < bre[j+1])
clase[indi]=j
}
return(clase)
}
# Get count of signal values around mean of open channels values
msoc=as.vector(by(train_data$signal, train_data$open_channels, mean))
freChann=function(x, chann){
indi= which(x >  msoc[chann] - 0.1 & x < msoc[chann] + 0.1)
return(length(indi))
}
# Smooth signal
mkSmoo=function(x, bw){
sk=ksmooth(seq(0, 10, length=length(x)), x, bandwidth = bw)
return(sk$y)
}
train_data %<>% groupby(batches) %>% summarise(
mKsmoo2 = mkSmoo(signal, bw=0.0025),
cls1 = decode(signal),
signalSquare = signal^2,
cls2=decode(signalSquare),
signale33 = sign(signal)*abs(signal)^(1/3),
cls3= decode(signale33)
)
train_data %<>% group_by(batches) %>% summarise(
mKsmoo2 = mkSmoo(signal, bw=0.0025),
cls1 = decode(signal),
signalSquare = signal^2,
cls2=decode(signalSquare),
signale33 = sign(signal)*abs(signal)^(1/3),
cls3= decode(signale33)
)
train_data %<>% group_by(batches) %>% mutate(
mKsmoo2 = mkSmoo(signal, bw=0.0025),
cls1 = decode(signal),
signalSquare = signal^2,
cls2=decode(signalSquare),
signale33 = sign(signal)*abs(signal)^(1/3),
cls3= decode(signale33)
)
warnings()
train_data %<>%
mutate(
RollingMaxB10 = roll_max(signal, n = 10, fill=max(signal)),
RollingMaxB20= roll_max(signal, n = 20, fill=max(signal))
) %>%
group_by(batches) %>% mutate(
RollingMeanB10 = roll_mean(signal, n = 10, fill=mean(signal)),
q4B = length(which(signal > 0 & signal < 1.5)==T),
mB =mean(signal),
maxB = max(signal),
RollingMaxB10 = roll_max(signal, n = 10, fill=max(signal)),
abs_avgBs2 = (abs(min(signal)) + abs(max(signal)))/2,
abs_avgB = (abs(min(signal)) + abs(max(signal)))/2,
q5B = length(which(signal > 0 & signal < 1.5)==T),
shiftM1 = shift(signal, n=1, fill=mean(signal), "lag"),
shiftM2 = shift(signal, n=2, fill=mean(signal), "lag"),
shiftP1 = shift(signal, n=1, fill=mean(signal), "lead"),
shiftP2 = shift(signal, n=2, fill=mean(signal), "lead")
)
View(train_data)
for(name in colnames(train_data)){
t = train_data[name]
train_data[is.na(t),name]=0
}
View(train_data)
train_data$trend_res<-0
train_data$trend_spl<-0
train_data$piece<-NA
for(lBatch in c(0,2:5,6:9)){
if(lBatch %in% c(6:9)){
trend.lm<-lm(signal~poly(time_batch,2),data=subset(train_data,batches==lBatch))
}else{
trend.lm<-lm(signal~time_batch,data=subset(train_data,batches==lBatch))
}
#cat("batch is",lBatch,'\n')
#summary(trend.lm) %>% print()
train_data$trend_res[train_data$batches==lBatch] <- trend.lm$residuals
train_data$trend_spl[train_data$batches==lBatch]<-trend.lm$fitted.values
train_data$piece[train_data$batches==lBatch]<-paste(as.character(lBatch),"1")
}
inds=train_data$batches==1 & train_data$time_batch<=10
trend.lm<-lm(signal~poly(time_batch,2),data=subset(train_data,inds))
train_data$trend_res[inds]<-trend.lm$residuals
train_data$trend_spl[inds]<-trend.lm$fitted.values
train_data$piece[inds]<-"11"
#summary(trend.lm) %>% print()
inds=train_data$batches==1 & train_data$time_batch>10
trend.lm<-lm(signal~time_batch,data=subset(train_data,inds))
train_data$trend_res[inds]<-trend.lm$residuals
train_data$trend_spl[inds]<-trend.lm$fitted.values
train_data$piece[inds]<-"12"
#summary(trend.lm) %>% print()
View(train_data)
cv5<- trainControl(method="cv",number=5,summaryFunction = f1)
train_ix<-createDataPartition(train_data$signal,p = 0.8,list = F)[,1]
train<-train_data[train_ix,]
test<-train_data[-train_ix,]
lda.model03=train(x=train_data %>% select(-time,-open_channels),y=train_data$open_channels,method="lda",family="binomial", trControl=cv5,verbosity=T,metric="F1")
summary(train)
train$trend_res
for(name in colnames(train_data)){
t = train_data[name]
train_data[is.na(t) | is.nan(t),name]=0
}
for(name in colnames(train_data)){
t = train_data[,name]
train_data[is.na(t) | is.nan(t),name]=0
}
for(name in colnames(train_data)){
t = train_data[,name] %>% unlist()
train_data[is.na(t) | is.nan(t),name]=0
}
train_ix<-createDataPartition(train_data$signal,p = 0.8,list = F)[,1]
train<-train_data[train_ix,]
test<-train_data[-train_ix,]
lda.model03=train(x=train %>% select(-time,-open_channels,-piece),
y=train$open_channels,method="lda",
family="binomial", trControl=cv5,
verbosity=T,metric="F1")
lda.valid03<-predict(lda.model03,newdata = test)
warnings()
lda.valid03<-predict(lda.model03,data = test)
lda.model03
lda.model03 <- train(open_channels~trend_res+batches,data=train,method="lda",family="binomial", trControl=cv5,verbosity=T,metric="F1")
warnings()
lda.valid03<-predict(lda.model03,data = test)
table(test$open_channels, lda.valid03)
lda.valid03
test$open_channels
lda.valid03<-predict(lda.model03,newdata = test)
table(test$open_channels, lda.valid03)
f1(data.frame(obs=test$open_channels,pred=lda.valid03))
train %>% select(-time,-open_channels,-piece)
lda.model03=train(x=train %>% select(-time,-open_channels,-piece),
y=train$open_channels,
method="lda",
family="binomial",
trControl=cv5,
verbosity=T,
metric="F1")
warnings()
lda.model03=train(x=train %>% select(-time,-open_channels,-piece) %>% as.matrix(),
y=train$open_channels,
method="lda",
family="binomial",
trControl=cv5,
verbosity=T,
metric="F1")
train$open_channels
train %>% select(-time,-open_channels,-piece) %>% as.matrix()
colnames(train)
lda.model03=train(open_channels~trend_res+trend_spl+time_batch,
data=train,
method="lda",
family="binomial",
trControl=cv5,
verbosity=T,
metric="F1")
warnings()
lda.valid03<-predict(lda.model03,newdata = test)
table(test$open_channels, lda.valid03)
f1(data.frame(obs=test$open_channels,pred=lda.valid03))
